{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f1c391-3fdf-4b02-ab00-df7b0b7112a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "ROOT_PATH=Path(\"..\").resolve().absolute()\n",
    "DEVICE=torch.device(\"cuda\")#torch.device(\"cpu\")#\n",
    "sys.path.append(str(ROOT_PATH))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9cc970-c2ae-4c2c-9691-06deeb294a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import main.module_base\n",
    "from audio_diffusion_pytorch import LogNormalDistribution\n",
    "import torch\n",
    "\n",
    "diffusion_sigma_distribution = LogNormalDistribution(mean=-3.0, std=1.0)\n",
    "    \n",
    "model_1 = main.module_base.Model(\n",
    "    learning_rate=1e-4,\n",
    "    beta1=0.9,\n",
    "    beta2=0.99,\n",
    "    in_channels=1,\n",
    "    channels=256,\n",
    "    patch_factor=16,\n",
    "    patch_blocks=1,\n",
    "    resnet_groups=8,\n",
    "    kernel_multiplier_downsample=2,\n",
    "    kernel_sizes_init=[1, 3, 7],\n",
    "    multipliers=[1, 2, 4, 4, 4, 4, 4],\n",
    "    factors=[4, 4, 4, 2, 2, 2],\n",
    "    num_blocks= [2, 2, 2, 2, 2, 2],\n",
    "    attentions= [False, False, False, True, True, True],\n",
    "    attention_heads=8,\n",
    "    attention_features=128,\n",
    "    attention_multiplier=2,\n",
    "    use_nearest_upsample=False,\n",
    "    use_skip_scale=True,\n",
    "    use_attention_bottleneck=True,\n",
    "    diffusion_sigma_distribution=diffusion_sigma_distribution,\n",
    "    diffusion_sigma_data=0.2,\n",
    "    diffusion_dynamic_threshold=0.0,\n",
    ")\n",
    "\n",
    "\n",
    "ckpt = torch.load(ROOT_PATH / \"data/checkpoints/piano.ckpt\", map_location=DEVICE)\n",
    "model_1.load_state_dict(ckpt[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbceac0-c4c2-481b-ad08-aaf24d430eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import main.module_base\n",
    "from audio_diffusion_pytorch import LogNormalDistribution\n",
    "import torch\n",
    "\n",
    "diffusion_sigma_distribution = LogNormalDistribution(mean=-3.0, std=1.0)\n",
    "    \n",
    "model_2 = main.module_base.Model(\n",
    "    learning_rate=1e-4,\n",
    "    beta1=0.9,\n",
    "    beta2=0.99,\n",
    "    in_channels=1,\n",
    "    channels=256,\n",
    "    patch_factor=16,\n",
    "    patch_blocks=1,\n",
    "    resnet_groups=8,\n",
    "    kernel_multiplier_downsample=2,\n",
    "    kernel_sizes_init=[1, 3, 7],\n",
    "    multipliers=[1, 2, 4, 4, 4, 4, 4],\n",
    "    factors=[4, 4, 4, 2, 2, 2],\n",
    "    num_blocks= [2, 2, 2, 2, 2, 2],\n",
    "    attentions= [False, False, False, True, True, True],\n",
    "    attention_heads=8,\n",
    "    attention_features=128,\n",
    "    attention_multiplier=2,\n",
    "    use_nearest_upsample=False,\n",
    "    use_skip_scale=True,\n",
    "    use_attention_bottleneck=True,\n",
    "    diffusion_sigma_distribution=diffusion_sigma_distribution,\n",
    "    diffusion_sigma_data=0.2,\n",
    "    diffusion_dynamic_threshold=0.0,\n",
    ")\n",
    "\n",
    "\n",
    "ckpt = torch.load(ROOT_PATH / \"data/checkpoints/bass.ckpt\", map_location=DEVICE)\n",
    "model_2.load_state_dict(ckpt[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac0dfe2-927a-421d-8ce2-c2f9fbd4be65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sampling_rate = 22050\n",
    "\n",
    "# @markdown Generation length in seconds (will be rounded to be a power of 2 of sample_rate*length)\n",
    "length = 10\n",
    "length_samples = 2**math.ceil(math.log2(length * sampling_rate))\n",
    "\n",
    "# @markdown Number of samples to generate\n",
    "num_samples = 1\n",
    "\n",
    "# @markdown Number of diffusion steps (higher tends to be better but takes longer to generate)\n",
    "num_steps = 10\n",
    "\n",
    "s1, sr1 = torchaudio.load('/data/5B/Piano_22050/test/Track01881.wav')\n",
    "s2, sr2 = torchaudio.load('/data/5B/Bass_22050/test/Track01881.wav')\n",
    "\n",
    "s1 = torchaudio.functional.resample(s1, orig_freq=sr1, new_freq=sampling_rate)\n",
    "s2 = torchaudio.functional.resample(s2, orig_freq=sr1, new_freq=sampling_rate)\n",
    "\n",
    "# display(Audio(s1, rate = sampling_rate))\n",
    "# display(Audio(s2, rate = sampling_rate))\n",
    "\n",
    "start_sample = 100 * sampling_rate\n",
    "s1 = s1.reshape(1, 1, -1)[:, :, start_sample:start_sample + length_samples]\n",
    "s2 = s2.reshape(1, 1, -1)[:, :, start_sample:start_sample + length_samples]\n",
    "m = s1+s2\n",
    "#m = m[:, :, start_sample:start_sample + length_samples]\n",
    "\n",
    "\n",
    "def plot_waves(s1, s2):\n",
    "    #plt.plot(s1.reshape(-1))\n",
    "    #plt.plot(s2.reshape(-1))\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    axes = fig.add_subplot(2,1,1)\n",
    "    spec, _, _, img = plt.specgram(s1.reshape(-1), NFFT=1024,)\n",
    "\n",
    "    axes = fig.add_subplot(2,1,2)\n",
    "    spec, _, _, img = plt.specgram(s2.reshape(-1), NFFT=1024,)\n",
    "\n",
    "    #plt.legend([\"piano\", \"bass\"])\n",
    "    plt.show()\n",
    "    \n",
    "    display(Audio(s1.reshape(1,-1), rate = sampling_rate))\n",
    "    display(Audio(s2.reshape(1,-1), rate = sampling_rate))\n",
    "    display(Audio((s1+s2).reshape(1,-1), rate = sampling_rate))\n",
    "    \n",
    "plot_waves(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29a83b-5634-4112-b4fb-b69403fcf6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.separation import KarrasSeparator, AEulerSeparator\n",
    "from audio_diffusion_pytorch.diffusion import KarrasSchedule\n",
    "\n",
    "@torch.no_grad()\n",
    "def separate(\n",
    "    model1,\n",
    "    model2,\n",
    "    mixture, \n",
    "    device: torch.device = torch.device(\"cuda\"), \n",
    "    num_steps:int = 100,\n",
    "):\n",
    "    \n",
    "    batch, in_channels = 1, 1\n",
    "    samples = mixture.shape[-1]\n",
    "\n",
    "    m = mixture.to(device)\n",
    "    models = [model1.model, model2.model]\n",
    "    \n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "        \n",
    "    sigma_sched = KarrasSchedule(sigma_min=1e-4, sigma_max=1.0, rho=9.0)\n",
    "    diffusion_separator = AEulerSeparator(mixture=m, delta=1.0)\n",
    "\n",
    "    sigma = sigma_sched(num_steps, device)\n",
    "    fns = [model.diffusion.denoise_fn for model in models]\n",
    "    noises = [torch.randn_like(m).to(device), torch.randn_like(m).to(device)]\n",
    "    return diffusion_separator.forward(noises=noises, fns=fns, sigmas=sigma, num_steps=num_steps)\n",
    "\n",
    "\n",
    "y1, y2 = separate(model_1, model_2, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58f9656-f741-4404-8245-fb09b944175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio((m).detach().cpu().view(1,-1), rate = sampling_rate))\n",
    "\n",
    "import torchmetrics.functional.audio as tma\n",
    "def si_snr(preds, targets):\n",
    "    return tma.scale_invariant_signal_noise_ratio(preds.cpu(), targets.cpu()).item()\n",
    "\n",
    "print(f\"SI-SNR (1): {si_snr(y1, s1)}\")\n",
    "print(f\"SI-SNR (2): {si_snr(y2, s2)}\")\n",
    "print(f\"SI-SNRi(1): {si_snr(y1, s1) - si_snr(s1, m)}\")\n",
    "print(f\"SI-SNRi (2): {si_snr(y2, s2) - si_snr(s2, m)}\")\n",
    "print(f\"SI-SNR (mix): {si_snr(y1+y2, m)}\")\n",
    "\n",
    "plot_waves(y1.cpu(), y2.cpu())\n",
    "plot_waves((s1.cpu()-y1.cpu()), (s2.cpu()-y2.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa157a0-2b03-4aba-b2d6-22aed2909796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from audio_diffusion_pytorch.diffusion import AEulerSampler, ADPM2Sampler, Diffusion, KarrasSchedule, Sampler, Schedule\n",
    "from audio_diffusion_pytorch.model import AudioDiffusionModel\n",
    "from audio_diffusion_pytorch.utils import default, exists\n",
    "\n",
    "class DiffusionSeparator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        diffusions: List[Diffusion],\n",
    "        *,\n",
    "        samplers: List[Sampler],\n",
    "        sigma_schedules: List[Schedule],\n",
    "        num_steps: Optional[int] = None,\n",
    "        nu = 2e-1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.denoise_fns = [diffusion.denoise_fn for diffusion in diffusions]\n",
    "        self.samplers = samplers\n",
    "        self.sigma_schedules = sigma_schedules\n",
    "        self.num_steps = num_steps\n",
    "        self.nu = nu\n",
    "\n",
    "    def forward(self, m: Tensor, noises: List[Tensor], num_steps: Optional[int] = None) -> List[Tensor]:\n",
    "        device = noises[0].device\n",
    "        num_steps = default(num_steps, self.num_steps)\n",
    "        assert exists(num_steps), \"Parameter `num_steps` must be provided\"\n",
    "        \n",
    "        # Compute sigmas using schedule\n",
    "        sigmas_list = [sigma_schedule(num_steps, device) for sigma_schedule in self.sigma_schedules]\n",
    "        \n",
    "        # Append additional kwargs to denoise function (used e.g. for conditional unet)\n",
    "        fns = self.denoise_fns\n",
    "        \n",
    "        # Separation procedure\n",
    "        xs = [sigmas[0] * noise for sigmas, noise in zip(sigmas_list, noises)]\n",
    "        \n",
    "        likelihood_steps = torch.tensor([self.nu]*len(sigmas_list[0]))\n",
    "        #likelihood_steps = compute_likelihood_steps(sigmas_list[0], gamma=2.25)\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def perform_sample_step(xs:list, step:int):\n",
    "            for j, x in enumerate(xs):\n",
    "                    yield self.samplers[j].step(\n",
    "                        x, fn=fns[j], sigma=sigmas_list[j][step], sigma_next=sigmas_list[j][step + 1])\n",
    "                    \n",
    "        # Denoise to sample\n",
    "        for step in range(num_steps - 1):\n",
    "            us = list(perform_sample_step(xs, step))\n",
    "            g_x = torch.stack(xs).sum(dim=0)\n",
    "            \n",
    "            #likelihood_steps = 1.0 / (torch.tensor([((2*(sigmas[:step])**2).sum()) for step in range(100)]) + 5.0)\n",
    "            #step_size = likelihood_steps[step]\n",
    "            step_size = self.nu #* (sigmas_list[0][step]**2) / sigmas_list[0][-1]**2\n",
    "            #step_size = sigmas_list[0][step]**2 / 0.012**2\n",
    "            likelihood = step_size*(m - g_x)\n",
    "            \n",
    "            if step % 20 == 0:\n",
    "                likelihood_norm = torch.norm(m - g_x)\n",
    "                print(\"likelihood norm:\", likelihood_norm.item())\n",
    "                print(\"step-size:\", step_size)\n",
    "                print(\"prior 1 norm:\", torch.norm(us[0] - xs[0]).item())\n",
    "                print(\"prior 2 norm:\", torch.norm(us[1] - xs[1]).item())\n",
    "                print(\"\")\n",
    "                #plot_waves(xs[0].cpu(), xs[1].cpu())\n",
    "            \n",
    "            for i in range(len(xs)):\n",
    "                xs[i] = us[i] + likelihood\n",
    "                \n",
    "        xs = [x.clamp(-1.0, 1.0) for x in xs]\n",
    "        print(likelihood_norm.item())\n",
    "        return xs\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def separate(\n",
    "    model1,\n",
    "    model2,\n",
    "    mixture, \n",
    "    device: torch.device = torch.device(\"cuda\"), \n",
    "    num_steps:int = 202,\n",
    "):\n",
    "    \n",
    "    batch, in_channels = 1, 1\n",
    "    samples = mixture.shape[-1]\n",
    "\n",
    "    m = torch.tensor(mixture).to(device)\n",
    "    models = [model1.model, model2.model]\n",
    "    \n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "    \n",
    "    #schedule = lambda num_steps,device: torch.flip(torch.sort(diffusion_sigma_distribution(num_steps*10, device))[0][::10],dims=[0])\n",
    "    #schedule = lambda num_steps, device: torch.arange(0.6, 0.0, -0.6/num_steps,device=device)\n",
    "    schedule = KarrasSchedule(sigma_min=1e-4, sigma_max=1.0, rho=8.0)\n",
    "    \n",
    "    diffusion_separator = DiffusionSeparator(\n",
    "        [model.diffusion for model in models],\n",
    "        samplers=[AEulerSampler(), AEulerSampler()],\n",
    "        sigma_schedules=[schedule, schedule],\n",
    "        num_steps=num_steps\n",
    "    )\n",
    "\n",
    "    noises = [torch.randn_like(m).to(device), torch.randn_like(m).to(device)]\n",
    "    return diffusion_separator.forward(m, noises)\n",
    "\n",
    "\n",
    "def compute_likelihood_steps(sigmas_rev: torch.tensor, gamma:float=1.0) -> torch.tensor:\n",
    "    sigmas = sigmas_rev.flip(dims=[0])\n",
    "    likelihood_steps = 1.0 / (torch.tensor([((2*(sigmas[:step])**2).sum()) for step in range(len(sigmas))]) + gamma**2)\n",
    "    return likelihood_steps.flip(dims=[0])\n",
    "\n",
    "\n",
    "y1, y2 = separate(model_1, model_2, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a28fb-78b3-4a3e-a18b-8ecfa6057930",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save(\"s1.wav\", s1.reshape(1,-1).cpu(), 22050)\n",
    "torchaudio.save(\"s2.wav\", s2.reshape(1,-1).cpu(), 22050)\n",
    "torchaudio.save(\"m.wav\", m.reshape(1,-1).cpu(), 22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7efc8-9dc3-4fd9-b53c-175d10aa1047",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e67b26-cfd6-4408-9c54-77ec37c13f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_data_pytorch import WAVDataset, AllTransform\n",
    "\n",
    "dataset_1 = WAVDataset(\n",
    "    \"/data/5B/Piano_22050/test\",\n",
    "    transforms=AllTransform(source_rate=22050, target_rate=22050,random_crop_size=262144, loudness=-20),\n",
    ")\n",
    "\n",
    "dataset_2 = WAVDataset(\n",
    "    \"/data/5B/Bass_22050/test\",\n",
    "    transforms=AllTransform(source_rate=22050, target_rate=22050, random_crop_size=262144, loudness=-20),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9984c882-c4aa-4a13-be29-8e316dbb7115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tqdm\n",
    "\n",
    "output_folder = Path(\"separations\")\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "for i in tqdm.autonotebook.tqdm(range(min(len(dataset_1), len(dataset_2)))):\n",
    "    x1, x2 = dataset_1[i], dataset_2[i]\n",
    "    x1 ,x2 = x1.unsqueeze(0), x2.unsqueeze(0)\n",
    "    \n",
    "    assert len(x1.shape) == len(x2.shape) == 3\n",
    "    assert x1.shape[0:2] == x2.shape[0:2] == (1,1)\n",
    "    assert x1.shape == x2.shape\n",
    "    \n",
    "    y1, y2 = separate(model_1, model_2, x1 + x2)\n",
    "    \n",
    "    separation_folder = (output_folder / f\"{i}\" )\n",
    "    separation_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "    torchaudio.save(separation_folder/ \"ori1.wav\", x1.reshape(1,-1).cpu(), 22050)\n",
    "    torchaudio.save(separation_folder/\"ori2.wav\", x2.reshape(1,-1).cpu(), 22050)\n",
    "    torchaudio.save(separation_folder /\"sep1.wav\", y1.reshape(1,-1).cpu(), 22050)\n",
    "    torchaudio.save(separation_folder /\"sep2.wav\", y2.reshape(1,-1).cpu(), 22050)\n",
    "    torchaudio.save(separation_folder /\"mix.wav\", m.reshape(1,-1).cpu(), 22050)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4188c9f0-6362-4f87-8010-b0d22deae18a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4930b28-ef80-4e25-becf-877706f213b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_diffusion_pytorch.diffusion import KarrasSampler, AEulerSampler, ADPM2Sampler, Diffusion, KarrasSchedule, Sampler, Schedule\n",
    "num_steps=100\n",
    "with torch.no_grad():\n",
    "     samples = model_1.model.sample(\n",
    "         noise=torch.randn((num_samples, 1, length_samples), device=DEVICE),\n",
    "         num_steps=num_steps,\n",
    "         sigma_schedule=KarrasSchedule(sigma_min=1e-4, sigma_max=1.0, rho=9.0),\n",
    "         sampler=ADPM2Sampler()\n",
    "     )\n",
    "\n",
    "# Log audio samples\n",
    "for i, sample in enumerate(samples):\n",
    "    import matplotlib.pyplot as plt\n",
    "    display(Audio(sample.cpu(), rate = sampling_rate))\n",
    "    spec, _, _, img = plt.specgram(sample.cpu().reshape(-1), NFFT=1024,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537665a1-5043-47a7-900e-2054eeaf847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def normal_pdf(x, mean:float=0.0, std: float = 1.0):\n",
    "    return torch.exp(-0.5*((x - mean)/std**2)**2)/(std * (2*torch.pi)**0.5)\n",
    "\n",
    "def lognormal_pdf(x:torch.Tensor, mean:float=0.0, std: float=1.0):\n",
    "     return torch.exp(-0.5*((torch.log(x) - mean)/std**2)**2)/(x * std * (2*torch.pi)**0.5)\n",
    "        \n",
    "        \n",
    "x = torch.arange(1e-4, 3, 0.001)\n",
    "normal_y = normal_pdf(x, 0 ,1)\n",
    "lognormal_y = lognormal_pdf(x, -3, 1)\n",
    "\n",
    "#plt.plot(x, torch.cumsum(lognormal_y,0))\n",
    "#lognormal_y\n",
    "plt.plot(torch.sort(diffusion_sigma_distribution(10000))[0][::10])\n",
    "\n",
    "plt.plot(KarrasSchedule(sigma_min=1e-4, sigma_max=3.0, rho=9.0)(1000,\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f2c22b-7544-4334-8cf1-b86d79c3c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = KarrasSchedule(sigma_min=1e-4, sigma_max=3.0, rho=9.0)\n",
    "sigmas_rev = k(num_steps=99, device=\"cpu\")\n",
    "\n",
    "def compute_likelihood_steps(sigmas_rev: torch.tensor) -> torch.tensor:\n",
    "    sigmas = sigmas_rev.flip(dims=[0])\n",
    "    likelihood_steps = 1.0 / (torch.tensor([((2*(sigmas[:step])**2).sum()) for step in range(len(sigmas))]) + 5.0)\n",
    "    return likelihood_steps.flip(dims=[0])\n",
    "\n",
    "#step_size = torch.tensor([3e-1]*100)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(sigmas)\n",
    "plt.plot(compute_likelihood_steps(sigmas_rev))\n",
    "plt.plot(step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f887384-922c-450a-8811-8d6b90cef287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_next = np.arange(1,100)/100\n",
    "x = np.arange(2,101)/100\n",
    "x_up = np.sqrt(x_next ** 2 * (x ** 2 - x_next ** 2) / x ** 2)\n",
    "x_down = np.sqrt(x_next ** 2 - x_up ** 2)\n",
    "\n",
    "#plt.plot(x_up)\n",
    "plt.plot(x_down - x)\n",
    "#plt.plot(x_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa8af75-e927-4677-9e05-063393eca234",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft=512\n",
    "hop_length=64\n",
    "speed_rate=1.5\n",
    "\n",
    "spec = torchaudio.transforms.Spectrogram(hop_length=hop_length,n_fft=n_fft, power=None)\n",
    "invspec = torchaudio.transforms.InverseSpectrogram(hop_length=hop_length,n_fft=n_fft)\n",
    "ts1 = torchaudio.transforms.TimeStretch(hop_length, n_freq=n_fft//2+1, fixed_rate=speed_rate)\n",
    "ts2 = torchaudio.transforms.TimeStretch(hop_length, n_freq=n_fft//2+1, fixed_rate=1/speed_rate)\n",
    "#ps1 = torchaudio.transforms.PitchShift(hop_length, n_freq=n_fft//2+1, fixed_rate=1/speed_rate)\n",
    "\n",
    "print(spec(s1).shape)\n",
    "s1_rec = invspec(ts2(ts1(spec(s1))))\n",
    "s1_rec = s1_rec[:,:,:length_samples]\n",
    "print(si_snr(s1_rec,s1))\n",
    "plot_waves(s1_rec,s1)\n",
    "#plt.plot((s1_rec-s1).reshape(-1))\n",
    "\n",
    "plt.plot(s1_rec.view(-1)[11000:11100:1])\n",
    "plt.plot(s1.view(-1)[11000:11100:1])\n",
    "\n",
    "print((s1_rec - s1).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08d9878-b0bb-4b82-99ce-5dcf631d79db",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c91bbd-7fa0-4180-931d-900d648e2b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
