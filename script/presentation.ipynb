{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Multi-source Diffusion Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Applications\n",
    "* **Music Generation** - Generate an audio track with respect of a learnt distribution\n",
    "* **Source Separation** - Separate audio tracks from a mixture\n",
    "* **Source Inpainting** - Given a single instrument track, generate other instruments coeherent tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "root_path = Path(\"..\").resolve().absolute()\n",
    "ckpts_path = Path(\"/home/irene/Documents/audio-diffusion-pytorch-trainer/logs/ckpts\")\n",
    "dataset_path = Path(\"/home/irene/Documents/audio-diffusion-pytorch-trainer/data/Slack/test/\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "sampling_rate = 22050\n",
    "length_samples = 262144\n",
    "num_steps = 200\n",
    "num_sources = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from misc import hparams, display_audio\n",
    "import main.module_base\n",
    "\n",
    "# load the diffusion models\n",
    "model = main.module_base.Model(**{**hparams, \"in_channels\": num_sources})\n",
    "model.load_state_dict(torch.load(ckpts_path/\"drums_piano_epoch=358.pt\"))\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Music Generation\n",
    "A simple generation algorithm using euler solver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_with_euler(noise, denoise_fn, sigmas):    \n",
    "    # Create initial noise\n",
    "    x = sigmas[0] * noise\n",
    "        \n",
    "    for i in range(len(sigmas) - 1):\n",
    "        sigma, sigma_next = sigmas[i], sigmas[i+1]\n",
    "        \n",
    "        # Compute derivative\n",
    "        d = (x - denoise_fn(x, sigma=sigma)) / sigma\n",
    "\n",
    "        # Euler method\n",
    "        x = x + d * (sigma_next - sigma)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually the addition of randomness helps with generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_with_euler(noise, denoise_fn, sigmas, s_churn):    \n",
    "    # Create initial noise\n",
    "    x = sigmas[0] * noise\n",
    "        \n",
    "    for i in range(len(sigmas) - 1):\n",
    "        sigma, sigma_next = sigmas[i], sigmas[i+1]\n",
    "        \n",
    "        # Inject randomness\n",
    "        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1)\n",
    "        sigma_hat = sigma * (gamma + 1)\n",
    "        x_hat = x + torch.randn_like(x) * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n",
    "        \n",
    "        # Compute derivative\n",
    "        d = (x_hat - denoise_fn(x_hat, sigma=sigma_hat)) / sigma_hat\n",
    "            \n",
    "        # Euler method\n",
    "        x = x_hat + d * (sigma_next - sigma_hat)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above algorithms it is then possible to generate an audio chunk from the learned distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from audio_diffusion_pytorch import KarrasSchedule\n",
    "\n",
    "# starting noise (batch-size, num-sources, num-elements)\n",
    "noise = torch.randn((1, num_sources, length_samples) , device=device) \n",
    "\n",
    "# denoise-function (approximates grad-log-prob)\n",
    "gradlogp_fn = model.model.diffusion.denoise_fn\n",
    "\n",
    "# solver timesteps distribution\n",
    "timesteps = KarrasSchedule(1e-4, 10.0, rho=9.0)(num_steps, device) # < much faster than linear  \n",
    "\n",
    "# sample from learned distribution\n",
    "y = sample_with_euler(noise, gradlogp_fn, timesteps, s_churn=10.0)\n",
    "\n",
    "# play sampled track (LEFT: drums, RIGHT: piano)\n",
    "display_audio(y, sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Source Separation\n",
    "Choose and load an audio track from the Slakh2100 test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_audio_track(tracks_dir: Path):\n",
    "    from ipywidgets import widgets\n",
    "    w1 = widgets.Combobox(\n",
    "        placeholder='Track00001.wav',\n",
    "        options=list(file.name for file in tracks_dir.glob(\"*.wav\")),\n",
    "        description='Audio track:',\n",
    "        ensure_option=True,\n",
    "        disabled=False,\n",
    "    )\n",
    "    w2 = widgets.FloatText(\n",
    "        value=100.0,\n",
    "        description=\"Start second:\",\n",
    "    )\n",
    "    return w1, w2\n",
    "\n",
    "track_widget, start_widget = choose_audio_track(dataset_path/\"bass\")\n",
    "display(track_widget)\n",
    "display(start_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from misc import load_audio, display_audio\n",
    "\n",
    "# load audio tracks\n",
    "start = round(start_widget.value * sampling_rate)\n",
    "end = start + length_samples\n",
    "signal_drums = load_audio(dataset_path / f'drums/{track_widget.value}', sampling_rate, start, end)\n",
    "signal_piano = load_audio(dataset_path / f'piano/{track_widget.value}', sampling_rate, start, end)\n",
    "\n",
    "# move signals to device\n",
    "signal_drums = signal_drums.to(device)\n",
    "signal_piano = signal_piano.to(device)\n",
    "mixture = signal_drums + signal_piano\n",
    "\n",
    "# display audio\n",
    "print(\"Drums Track:\")\n",
    "display_audio(signal_drums, sampling_rate)\n",
    "\n",
    "print(\"\\nPiano Track:\")\n",
    "display_audio(signal_piano, sampling_rate)\n",
    "\n",
    "print(\"\\nMixture:\")\n",
    "display_audio(mixture, sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separation algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def separate_with_euler(noise, denoise_fn, sigmas, mixture, s_churn=1.0):  \n",
    "    # initial noise\n",
    "    x = sigmas[0] * noise\n",
    "        \n",
    "    for i in range(len(sigmas) - 1):\n",
    "        sigma, sigma_next = sigmas[i], sigmas[i+1]\n",
    "        \n",
    "        # inject randomness\n",
    "        gamma = min(s_churn / (len(sigmas) - 1), 2 ** 0.5 - 1)\n",
    "        sigma_hat = sigma * (gamma + 1)\n",
    "        x_hat = x + torch.randn_like(x) * (sigma_hat ** 2 - sigmas[i] ** 2) ** 0.5\n",
    "        \n",
    "        # Compute conditioned derivative\n",
    "        x_hat[:, 1, :] = mixture + torch.randn_like(mixture) * sigma - x_hat[:, 0, :]\n",
    "        score = (x_hat - denoise_fn(x_hat, sigma=sigma_hat)) / sigma_hat\n",
    "        score_drums, score_piano = score[:,:1], score[:,1:]\n",
    "        d = score_drums - score_piano\n",
    "            \n",
    "        # Euler method\n",
    "        x = x_hat + d * (sigma_next - sigma_hat)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separation of input audio mixture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from main.separation import separate\n",
    "\n",
    "# starting noise (batch-size, num-sources, num-elements)\n",
    "noise = torch.randn((1, num_sources, length_samples), device=device) \n",
    "\n",
    "# denoise-function (approximates grad-log-prob)\n",
    "gradlogp_fn = model.model.diffusion.denoise_fn\n",
    "\n",
    "# solver timesteps distribution\n",
    "timesteps = KarrasSchedule(1e-4, 10.0, rho=9.0)(num_steps, device) \n",
    "\n",
    "# sample from learned distribution\n",
    "y = separate_with_euler(noise, gradlogp_fn, timesteps, mixture)\n",
    "y_drums, y_piano = y[:,:1,:], y[:,1:, :]\n",
    "\n",
    "# display audio\n",
    "print(\"Separated Drums Track:\")\n",
    "display_audio(y_drums, sampling_rate)\n",
    "                      \n",
    "print(\"\\nSeparated Piano Track:\")\n",
    "display_audio(y_piano, sampling_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
