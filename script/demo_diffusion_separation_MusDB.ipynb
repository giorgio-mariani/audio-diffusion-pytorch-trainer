{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f1c391-3fdf-4b02-ab00-df7b0b7112a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "ROOT_PATH=Path(\"..\").resolve().absolute()\n",
    "DEVICE=torch.device(\"cuda\")#torch.device(\"cpu\")\n",
    "sys.path.append(str(ROOT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9cc970-c2ae-4c2c-9691-06deeb294a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import main.module_base\n",
    "from audio_diffusion_pytorch import LogNormalDistribution\n",
    "import torch\n",
    "\n",
    "diffusion_sigma_distribution = LogNormalDistribution(mean=-3.0, std=1.0)\n",
    "    \n",
    "model_1 = main.module_base.Model(\n",
    "    learning_rate=1e-4,\n",
    "    beta1=0.9,\n",
    "    beta2=0.99,\n",
    "    in_channels=2,\n",
    "    channels=256,\n",
    "    patch_factor=32,\n",
    "    patch_blocks=1,\n",
    "    resnet_groups=8,\n",
    "    kernel_multiplier_downsample=2,\n",
    "    kernel_sizes_init=[1, 3, 7],\n",
    "    multipliers=[1, 2, 4, 4, 4, 4, 4],\n",
    "    factors=[4, 4, 4, 2, 2, 2],\n",
    "    num_blocks= [2, 2, 2, 2, 2, 2],\n",
    "    attentions= [False, False, False, True, True, True],\n",
    "    attention_heads=8,\n",
    "    attention_features=128,\n",
    "    attention_multiplier=2,\n",
    "    use_nearest_upsample=False,\n",
    "    use_skip_scale=True,\n",
    "    use_attention_bottleneck=True,\n",
    "    diffusion_sigma_distribution=diffusion_sigma_distribution,\n",
    "    diffusion_sigma_data=0.2,\n",
    "    diffusion_dynamic_threshold=0.0,\n",
    ")\n",
    "\n",
    "\n",
    "#ckpt = torch.load(ROOT_PATH / \"data/checkpoints/piano.ckpt\", map_location=DEVICE)\n",
    "ckpt = torch.load(\"../logs/ckpts/2022-11-25-15-45-59/epoch=1285-valid_loss=0.018.ckpt\", map_location=DEVICE)\n",
    "\n",
    "#ckpt = torch.load(\"../logs/ckpts/2022-11-17-15-56-09/epoch=208-valid_loss=0.053.ckpt\", map_location=DEVICE)\n",
    "model_1.load_state_dict(ckpt[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbceac0-c4c2-481b-ad08-aaf24d430eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import main.module_base\n",
    "from audio_diffusion_pytorch import LogNormalDistribution\n",
    "import torch\n",
    "\n",
    "diffusion_sigma_distribution = LogNormalDistribution(mean=-3.0, std=1.0)\n",
    "    \n",
    "model_2 = main.module_base.Model(\n",
    "    learning_rate=1e-4,\n",
    "    beta1=0.9,\n",
    "    beta2=0.99,\n",
    "    in_channels=1,\n",
    "    channels=256,\n",
    "    patch_factor=16,\n",
    "    patch_blocks=1,\n",
    "    resnet_groups=8,\n",
    "    kernel_multiplier_downsample=2,\n",
    "    kernel_sizes_init=[1, 3, 7],\n",
    "    multipliers=[1, 2, 4, 4, 4, 4, 4],\n",
    "    factors=[4, 4, 4, 2, 2, 2],\n",
    "    num_blocks= [2, 2, 2, 2, 2, 2],\n",
    "    attentions= [False, False, False, True, True, True],\n",
    "    attention_heads=8,\n",
    "    attention_features=128,\n",
    "    attention_multiplier=2,\n",
    "    use_nearest_upsample=False,\n",
    "    use_skip_scale=True,\n",
    "    use_attention_bottleneck=True,\n",
    "    diffusion_sigma_distribution=diffusion_sigma_distribution,\n",
    "    diffusion_sigma_data=0.2,\n",
    "    diffusion_dynamic_threshold=0.0,\n",
    ")\n",
    "\n",
    "ckpt = torch.load(ROOT_PATH / \"data/checkpoints/piano.ckpt\", map_location=DEVICE)\n",
    "#ckpt = torch.load(\"/data/MusDB/checkpoints/drums/drums-musdb.ckpt\", map_location=DEVICE)\n",
    "#ckpt = torch.load(\"../logs/ckpts/2022-11-22-18-14-22/epoch=10571-valid_loss=0.200.ckpt\", map_location=DEVICE)\n",
    "model_2.load_state_dict(ckpt[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac0dfe2-927a-421d-8ce2-c2f9fbd4be65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "import math\n",
    "from audio_data_pytorch.transforms import Loudness\n",
    "sampling_rate = 22050\n",
    "\n",
    "# @markdown Generation length in seconds (will be rounded to be a power of 2 of sample_rate*length)\n",
    "#length = 10\n",
    "length_samples = 262144#2**math.ceil(math.log2(length * sampling_rate))\n",
    "\n",
    "# @markdown Number of samples to generate\n",
    "num_samples = 1\n",
    "\n",
    "# @markdown Number of diffusion steps (higher tends to be better but takes longer to generate)\n",
    "num_steps = 10\n",
    "\n",
    "#s1, sr1 = torchaudio.load('/data/MusDB/data/test_MusDB_reduced/Al James - Schoolboy Facination/bass.wav')\n",
    "#s2, sr2 = torchaudio.load('/data/MusDB/data/test_MusDB_reduced/Al James - Schoolboy Facination/drums.wav')\n",
    "\n",
    "  #s1, sr1 = torchaudio.load('/data/5B/Bass_22050/test/Track01881.wav')\n",
    "#s2, sr2 = torchaudio.load('/data/5B/Piano_22050/test/Track01881.wav')\n",
    "s1, sr1 = torchaudio.load('/data/MusDB/data/test_MusDB_reduced/Angels In Amplifiers - I\\'m Alright/bass.wav')\n",
    "s2, sr2 = torchaudio.load('/data/MusDB/data/test_MusDB_reduced/Angels In Amplifiers - I\\'m Alright/drums.wav')\n",
    "\n",
    "if not (sr1 == sr2 == sampling_rate):\n",
    "    print(f\"Inconsistent sampling rate: {sr1}, {sr2}, {sampling_rate}\")\n",
    "\n",
    "s1 = torchaudio.functional.resample(s1, orig_freq=sr1, new_freq=sampling_rate)\n",
    "s2 = torchaudio.functional.resample(s2, orig_freq=sr2, new_freq=sampling_rate)\n",
    "#s1 = Loudness(sampling_rate,-20.0)(s1)\n",
    "#s2 = Loudness(sampling_rate, -20.0)(s2)\n",
    "\n",
    "#convert to mono\n",
    "s1, s2 = s1.mean(dim=0, keepdims=True), s2.mean(dim=0, keepdims=True)\n",
    "\n",
    "# display(Audio(s1, rate = sampling_rate))\n",
    "# display(Audio(s2, rate = sampling_rate))\n",
    "\n",
    "start_sample = 100 * sampling_rate\n",
    "s1 = s1.reshape(1, 1, -1)[:, :, start_sample:start_sample + length_samples]\n",
    "s2 = s2.reshape(1, 1, -1)[:, :, start_sample:start_sample + length_samples]\n",
    "m = s1+s2\n",
    "#m = m[:, :, start_sample:start_sample + length_samples]\n",
    "\n",
    "def plot_waves(s1, s2):\n",
    "    import matplotlib.pyplot as plt\n",
    "    #plt.plot(s1.reshape(-1))\n",
    "    #plt.plot(s2.reshape(-1))\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    axes = fig.add_subplot(2,1,1)\n",
    "    spec, _, _, img = plt.specgram(s1.reshape(-1), NFFT=1024,)\n",
    "\n",
    "    axes = fig.add_subplot(2,1,2)\n",
    "    spec, _, _, img = plt.specgram(s2.reshape(-1), NFFT=1024,)\n",
    "\n",
    "    #plt.legend([\"piano\", \"bass\"])\n",
    "    plt.show()\n",
    "    \n",
    "    display(Audio(s1.reshape(1,-1), rate = sampling_rate))\n",
    "    display(Audio(s2.reshape(1,-1), rate = sampling_rate))\n",
    "    display(Audio((s1+s2).reshape(1,-1), rate = sampling_rate))\n",
    "    \n",
    "plot_waves(s1, s2)\n",
    "\n",
    "spec = torchaudio.transforms.Spectrogram(n_fft=1024, power=None)\n",
    "invspec = torchaudio.transforms.InverseSpectrogram(n_fft=1024)\n",
    "ts = torchaudio.transforms.TimeStretch(1024/2, n_freq=513, fixed_rate=1.4)\n",
    "#plot_waves(invspec(ts(spec(s1))),s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa157a0-2b03-4aba-b2d6-22aed2909796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from audio_diffusion_pytorch.diffusion import AEulerSampler, ADPM2Sampler, Diffusion, KarrasSchedule, Sampler, Schedule\n",
    "from audio_diffusion_pytorch.model import AudioDiffusionModel\n",
    "from audio_diffusion_pytorch.utils import default, exists\n",
    "\n",
    "class DiffusionSeparator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        diffusions: List[Diffusion],\n",
    "        *,\n",
    "        samplers: List[Sampler],\n",
    "        sigma_schedules: List[Schedule],\n",
    "        num_steps: Optional[int] = None,\n",
    "        nu = 0.2#2e-5 / 0.01**2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.denoise_fns = [denoise_fn for denoise_fn in diffusions]\n",
    "        self.samplers = samplers\n",
    "        self.sigma_schedules = sigma_schedules\n",
    "        self.num_steps = num_steps\n",
    "        self.nu = nu\n",
    "\n",
    "    def forward(self, m: Tensor, noises: List[Tensor], num_steps: Optional[int] = None) -> List[Tensor]:\n",
    "        device = noises[0].device\n",
    "        num_steps = default(num_steps, self.num_steps)\n",
    "        assert exists(num_steps), \"Parameter `num_steps` must be provided\"\n",
    "        \n",
    "        # Compute sigmas using schedule\n",
    "        sigmas_list = [sigma_schedule(num_steps, device) for sigma_schedule in self.sigma_schedules]\n",
    "        \n",
    "        # Append additional kwargs to denoise function (used e.g. for conditional unet)\n",
    "        fns = self.denoise_fns\n",
    "        \n",
    "        # Separation procedure\n",
    "        xs = [sigmas[0] * noise for sigmas, noise in zip(sigmas_list, noises)]\n",
    "                \n",
    "        #likelihood_steps = torch.tensor([self.nu]*len(sigmas_list[0]))\n",
    "        #likelihood_steps = compute_likelihood_steps(sigmas_list[0], gamma=0.0)\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def perform_sample_step(xs:list, step:int):\n",
    "            for j, x in enumerate(xs):\n",
    "                    yield self.samplers[j].step(\n",
    "                        x, fn=fns[j], sigma=sigmas_list[j][step], sigma_next=sigmas_list[j][step + 1])\n",
    "                    \n",
    "        # Denoise to sample\n",
    "        for step in range(num_steps - 1):\n",
    "            us = list(perform_sample_step(xs, step))\n",
    "            g_x = torch.stack(xs).sum(dim=0)\n",
    "            \n",
    "            step_size = self.nu\n",
    "            likelihood = step_size*(m - g_x)\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                likelihood_norm = torch.norm(m - g_x)\n",
    "                print(\"likelihood norm:\", likelihood_norm.item())\n",
    "                print(\"step-size:\", step_size)\n",
    "                print(\"prior 1 norm:\", torch.norm(us[0] - xs[0]).item())\n",
    "                print(\"prior 2 norm:\", torch.norm(us[1] - xs[1]).item())\n",
    "                print(\"\")\n",
    "                plot_waves(xs[0].cpu(), xs[1].cpu())\n",
    "            \n",
    "            for i in range(len(xs)):\n",
    "                xs[i] = us[i] + likelihood\n",
    "                \n",
    "        xs = [x.clamp(-1.0, 1.0) for x in xs]\n",
    "        print(likelihood_norm.item())\n",
    "        return xs\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def separate(\n",
    "    model1,\n",
    "    model2,\n",
    "    mixture, \n",
    "    device: torch.device = torch.device(\"cuda\"), \n",
    "    num_steps:int = 402,\n",
    "):\n",
    "    \n",
    "    batch, in_channels = 1, 1\n",
    "    samples = mixture.shape[-1]\n",
    "\n",
    "    m = torch.tensor(mixture).to(device)\n",
    "    models = [model1.model, model2.model]\n",
    "    \n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "        \n",
    "    #schedule = lambda num_steps,device: torch.flip(torch.sort(diffusion_sigma_distribution(num_steps*10, device))[0][::10],dims=[0])\n",
    "    #schedule = lambda num_steps, device: torch.arange(1.0, 0.0, -1.0/num_steps,device=device)\n",
    "    schedule = KarrasSchedule(sigma_min=1e-4, sigma_max=1.0, rho=9.0)\n",
    "    \n",
    "    fn = lambda x, sigma: model_1.model.diffusion.denoise_fn(x.repeat(1,2,1), sigma=sigma).mean()\n",
    "    \n",
    "    diffusion_separator = DiffusionSeparator(\n",
    "        [fn, model_2.model.diffusion.denoise_fn],#[model.diffusion.denoise_fn for model in models],\n",
    "        samplers=[ADPM2Sampler(rho=1.0), ADPM2Sampler(rho=1.0)],\n",
    "        sigma_schedules=[schedule, schedule],\n",
    "        num_steps=num_steps\n",
    "    )\n",
    "    noises = [torch.randn_like(m).to(device), torch.randn_like(m).to(device)]\n",
    "    return diffusion_separator.forward(m, noises)\n",
    "\n",
    "\n",
    "y1, y2 = separate(model_1, model_2, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58f9656-f741-4404-8245-fb09b944175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(y1.detach().cpu().view(1,-1), rate = sampling_rate))\n",
    "display(Audio(y2.detach().cpu().view(1,-1), rate = sampling_rate))\n",
    "display(Audio((y1+y2).detach().cpu().view(1,-1), rate = sampling_rate))\n",
    "display(Audio((m).detach().cpu().view(1,-1), rate = sampling_rate))\n",
    "\n",
    "import torchmetrics.functional.audio as tma\n",
    "def si_snr(preds, targets):\n",
    "    return tma.scale_invariant_signal_noise_ratio(preds.cpu(), targets.cpu()).item()\n",
    "\n",
    "y1 = y1.mean(dim=1, keepdims=True)\n",
    "y2 = y2.mean(dim=1, keepdims=True)\n",
    "print(f\"SI-SNR (1): {si_snr(y1, s1)}\")\n",
    "print(f\"SI-SNR (2): {si_snr(y2, s2)}\")\n",
    "print(f\"SI-SNRi(1): {si_snr(y1, s1) - si_snr(s1, m)}\")\n",
    "print(f\"SI-SNRi (2): {si_snr(y2, s2) - si_snr(s2, m)}\")\n",
    "print(f\"SI-SNR (mix): {si_snr(y1+y2, m)}\")\n",
    "\n",
    "plot_waves(y1.cpu(), y2.cpu())\n",
    "plot_waves((s1.cpu()-y1.cpu()), (s2.cpu()-y2.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a28fb-78b3-4a3e-a18b-8ecfa6057930",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save(\"s1.wav\", s1.reshape(1,-1).cpu(), 22050)\n",
    "torchaudio.save(\"s2.wav\", s2.reshape(1,-1).cpu(), 22050)\n",
    "torchaudio.save(\"m.wav\", m.reshape(1,-1).cpu(), 22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7efc8-9dc3-4fd9-b53c-175d10aa1047",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e67b26-cfd6-4408-9c54-77ec37c13f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_data_pytorch import WAVDataset, AllTransform\n",
    "\n",
    "dataset_1 = WAVDataset(\n",
    "    \"/data/5B/Piano_22050/test\",\n",
    "    transforms=AllTransform(source_rate=22050, target_rate=22050,random_crop_size=262144, loudness=-20),\n",
    ")\n",
    "\n",
    "dataset_2 = WAVDataset(\n",
    "    \"/data/5B/Bass_22050/test\",\n",
    "    transforms=AllTransform(source_rate=22050, target_rate=22050, random_crop_size=262144, loudness=-20),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9984c882-c4aa-4a13-be29-8e316dbb7115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tqdm\n",
    "\n",
    "output_folder = Path(\"separations\")\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "for i in tqdm.autonotebook.tqdm(range(min(len(dataset_1), len(dataset_2)))):\n",
    "    x1, x2 = dataset_1[i], dataset_2[i]\n",
    "    x1 ,x2 = x1.unsqueeze(0), x2.unsqueeze(0)\n",
    "    \n",
    "    assert len(x1.shape) == len(x2.shape) == 3\n",
    "    assert x1.shape[0:2] == x2.shape[0:2] == (1,1)\n",
    "    assert x1.shape == x2.shape\n",
    "    \n",
    "    y1, y2 = separate(model_1, model_2, x1 + x2)\n",
    "    \n",
    "    separation_folder = (output_folder / f\"{i}\" )\n",
    "    separation_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "    torchaudio.save(separation_folder/ \"ori1.wav\", x1.reshape(1,-1).cpu(), 22050)\n",
    "    torchaudio.save(separation_folder/\"ori2.wav\", x2.reshape(1,-1).cpu(), 22050)\n",
    "    torchaudio.save(separation_folder /\"sep1.wav\", y1.reshape(1,-1).cpu(), 22050)\n",
    "    torchaudio.save(separation_folder /\"sep2.wav\", y2.reshape(1,-1).cpu(), 22050)\n",
    "    torchaudio.save(separation_folder /\"mix.wav\", m.reshape(1,-1).cpu(), 22050)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4188c9f0-6362-4f87-8010-b0d22deae18a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4930b28-ef80-4e25-becf-877706f213b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_diffusion_pytorch.diffusion import AEulerSampler, ADPM2Sampler, Diffusion, KarrasSchedule, Sampler, Schedule\n",
    "\n",
    "with torch.no_grad():\n",
    "     samples = model_1.model.sample(\n",
    "         noise=torch.randn((num_samples, 2, length_samples), device=DEVICE),\n",
    "         num_steps=100,\n",
    "         sigma_schedule=KarrasSchedule(sigma_min=1e-4, sigma_max=3.0, rho=7.0),\n",
    "         sampler=ADPM2Sampler(),\n",
    "     )\n",
    "\n",
    "# Log audio samples\n",
    "for i, sample in enumerate(samples):\n",
    "     display(Audio(sample.cpu(), rate = sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f2c22b-7544-4334-8cf1-b86d79c3c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = KarrasSchedule(sigma_min=1e-4, sigma_max=3.0, rho=9.0)\n",
    "sigmas_rev = k(num_steps=99, device=\"cpu\")\n",
    "\n",
    "#step_size = torch.tensor([3e-1]*100)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(sigmas)\n",
    "plt.plot(compute_likelihood_steps(sigmas_rev, gamma=0.0128))\n",
    "#plt.plot(step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36a06fc-8df9-4e85-aa09-3f731b23551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_diffusion_pytorch.diffusion import KarrasSchedule\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_wave(s1):    \n",
    "    fig = plt.figure()\n",
    "    axes = fig.add_subplot(2,1,1)\n",
    "    spec, _, _, img = plt.specgram(s1.reshape(-1), NFFT=1024,)\n",
    "    plt.show()\n",
    "    \n",
    "    display(Audio(s1.reshape(1,-1), rate = sampling_rate))\n",
    "\n",
    "k = KarrasSchedule(sigma_min=1e-4, sigma_max=3.0, rho=9.0)\n",
    "sigmas = k(num_steps=10, device=\"cpu\")\n",
    "\n",
    "plt.plot(sigmas)\n",
    "noise = torch.randn_like(s1).cpu()\n",
    "\n",
    "for sigma in sigmas:\n",
    "    print(f\"({sigma}) SI-SNR : {si_snr(sigma*noise + s1, s1)}\")\n",
    "    #plot_wave(sigma*noise + s1)\n",
    "    print(\"-----------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ec0c51-2e06-4778-a789-4dde1eb25618",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(100)/1000\n",
    "y=10*np.log10(np.linalg.norm(s1)**2 / (s1.numel()*x**2))\n",
    "#plt.plot(x,y)\n",
    "import plotly.express as px\n",
    "px.line(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e025ff45-d6d1-47a9-b901-1e66665a992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from IPython.display import Audio\n",
    "from audio_diffusion_pytorch.diffusion import ADPM2Sampler, KarrasSchedule\n",
    "sampling_rate=22050\n",
    "\n",
    "device=\"cuda\"\n",
    "with torch.no_grad():\n",
    "    \n",
    "    model = model_2.model\n",
    "    model.to(device)\n",
    "        \n",
    "    samples = model.sample(\n",
    "         noise=torch.randn((1, 1, 2**math.ceil(math.log2(10 * sampling_rate))), device='cuda'),\n",
    "         num_steps=100,\n",
    "         sigma_schedule=KarrasSchedule(\n",
    "             sigma_min=1e-4,\n",
    "             sigma_max=1.0,\n",
    "             rho=9.0\n",
    "         ),\n",
    "         sampler=ADPM2Sampler(rho=1.0),\n",
    "     )\n",
    "\n",
    "# Log audio samples\n",
    "for i, sample in enumerate(samples):\n",
    "    display(Audio(sample.cpu(), rate = sampling_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
