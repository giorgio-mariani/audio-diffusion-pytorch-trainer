{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "ROOT_PATH=Path(\"..\").resolve().absolute()\n",
    "DEVICE=torch.device(\"cuda\")#torch.device(\"cpu\")\n",
    "sys.path.append(str(ROOT_PATH))\n",
    "\n",
    "import hydra\n",
    "from audio_diffusion_pytorch import AudioDiffusionModel\n",
    "import torch\n",
    "from main import utils\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "\n",
    "hydra.initialize(config_path=\"..\", job_name=\"test_app\")\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "# @markdown Both models are equal in size and trained on a variety of music. The 718k version tends to generate piano sounds more often than generic music, where 720k tends to generate orchestra songs more often.\n",
    "version = \"1136_720k\" #@param [\"1136_718k\", \"1136_720k\"]\n",
    "model = torch.hub.load_state_dict_from_url(\n",
    "   f'https://huggingface.co/archinetai/audio-diffusion-pytorch/resolve/main/audio_{version}.pt', map_location='cuda')\n",
    "\n",
    "#with open(\"../data/audio_1136_700k.pt\", \"rb\") as f:\n",
    "#    model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = hydra.compose(config_name=\"config\", overrides=[\"exp=base_slakh_1\"])\n",
    "#print(OmegaConf.to_yaml(cfg))\n",
    "utils.extras(cfg)\n",
    "\n",
    "model_1 = hydra.utils.instantiate(cfg.model, _convert_=\"partial\")\n",
    "model_2 = hydra.utils.instantiate(cfg.model, _convert_=\"partial\")\n",
    "\n",
    "ckpt1 = torch.load('../logs/ckpts/drums_slakh/epoch=506-valid_loss=0.043.ckpt')\n",
    "model_1.load_state_dict(ckpt1['state_dict'])\n",
    "#del ckpt1\n",
    "\n",
    "#ckpt2 = torch.load('../logs/ckpts/guitar_slakh/epoch=987-valid_loss=0.008.ckpt')\n",
    "model_2.load_state_dict(ckpt1['state_dict'])\n",
    "#del ckpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate Samples\n",
    "from audio_diffusion_pytorch import KarrasSchedule, AEulerSampler\n",
    "from IPython.display import Audio\n",
    "import math\n",
    "\n",
    "sampling_rate = 22050\n",
    "\n",
    "# @markdown Generation length in seconds (will be rounded to be a power of 2 of sample_rate*length)\n",
    "length = 10 #@param {type: \"slider\", min: 1, max: 87, step: 1}\n",
    "length_samples = 2**math.ceil(math.log2(length * sampling_rate))\n",
    "\n",
    "# @markdown Number of samples to generate\n",
    "num_samples = 1 #@param {type: \"slider\", min: 1, max: 16, step: 1}\n",
    "\n",
    "# @markdown Number of diffusion steps (higher tends to be better but takes longer to generate)\n",
    "num_steps = 100 #@param {type: \"slider\", min: 1, max: 200, step: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     samples = model.sample(\n",
    "#         noise=torch.randn((num_samples, 2, 2 ** length_samples), device='cuda'),\n",
    "#         num_steps=num_steps,\n",
    "#         sigma_schedule=KarrasSchedule(\n",
    "#             sigma_min=1e-4,\n",
    "#             sigma_max=10.0,\n",
    "#             rho=7.0\n",
    "#         ),\n",
    "#         sampler=AEulerSampler(),\n",
    "#     )\n",
    "#\n",
    "# # Log audio samples\n",
    "# for i, sample in enumerate(samples):\n",
    "#     display(Audio(sample.cpu(), rate = sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1, sr1 = torchaudio.load('../data/drums_22050/validation/validation/Track01501.wav')\n",
    "s2, sr2 = torchaudio.load('../data/guitar_22050/validation/validation/Track01501.wav')\n",
    "assert sr1 == sampling_rate\n",
    "assert sr2 == sampling_rate\n",
    "# np.load(\"../data/guitar_22050/source2_48K.npy\")\n",
    "\n",
    "# display(Audio(s1, rate = sampling_rate))\n",
    "# display(Audio(s2, rate = sampling_rate))\n",
    "\n",
    "# np.save(\"source1_48K\", samples[0].cpu())\n",
    "# np.save(\"source2_48K\", samples[2].cpu())\n",
    "\n",
    "s1 = s1.reshape(1, 1, -1)\n",
    "s2 = s2.reshape(1, 1, -1)\n",
    "m = s1+s2\n",
    "\n",
    "start_sample = 34 * sampling_rate\n",
    "m = m[:, :, start_sample:start_sample + length_samples]\n",
    "\n",
    "display(Audio(m.reshape(1,-1), rate = sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from audio_diffusion_pytorch.diffusion import AEulerSampler, Diffusion, KarrasSchedule, Sampler, Schedule\n",
    "from audio_diffusion_pytorch.model import AudioDiffusionModel\n",
    "from audio_diffusion_pytorch.utils import default, exists\n",
    "\n",
    "\n",
    "class DiffusionSeparator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        diffusions: List[Diffusion],\n",
    "        *,\n",
    "        samplers: List[Sampler],\n",
    "        sigma_schedules: List[Schedule],\n",
    "        num_steps: Optional[int] = None,\n",
    "        eta: int = 0.0005\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.denoise_fns = [diffusion.denoise_fn for diffusion in diffusions]\n",
    "        self.samplers = samplers\n",
    "        self.sigma_schedules = sigma_schedules\n",
    "        self.num_steps = num_steps\n",
    "        self.eta = eta\n",
    "\n",
    "    def forward(self, m: Tensor, noises: List[Tensor], num_steps: Optional[int] = None, **kwargs) -> List[Tensor]:\n",
    "        device = noises[0].device\n",
    "        num_steps = default(num_steps, self.num_steps)  # type: ignore\n",
    "        assert exists(num_steps), \"Parameter `num_steps` must be provided\"\n",
    "        \n",
    "        # Compute sigmas using schedule\n",
    "        sigmas_list = [sigma_schedule(num_steps, device) for sigma_schedule in self.sigma_schedules]\n",
    "        \n",
    "        # Append additional kwargs to denoise function (used e.g. for conditional unet)\n",
    "        fns = [lambda *a, **ka: denoise_fn(*a, **{**ka, **kwargs}) for denoise_fn in self.denoise_fns]  # noqa\n",
    "        \n",
    "        # Separation procedure\n",
    "        xs = [sigmas[0] * noise for sigmas, noise in zip(sigmas_list, noises)]\n",
    "        \n",
    "        @torch.no_grad()\n",
    "        def perform_sample_step(xs:list, step:int):\n",
    "            for j, x in enumerate(xs):\n",
    "                    yield self.samplers[j].step(\n",
    "                        x, \n",
    "                        fn=fns[j], \n",
    "                        sigma=sigmas_list[j][step], \n",
    "                        sigma_next=sigmas_list[j][step + 1])\n",
    "        \n",
    "        # Denoise to sample\n",
    "        for i in range(num_steps - 1):\n",
    "            \n",
    "            # update with respect to the prior\n",
    "            xs = list(perform_sample_step(xs, i))\n",
    "            xs = [x.detach() for x in xs]\n",
    "            \n",
    "            # compute likelihood function\n",
    "            # first perturb m with forward noise (works only if sigma scheduler is always the same)\n",
    "            m_i = m # + torch.randn_like(m) * sigmas_list[0][i]\n",
    "            #m_i = m[0:1,0:1]\n",
    "            for x in xs:\n",
    "                x.requires_grad = True\n",
    "            \n",
    "            sum_x = torch.stack(xs).sum(dim=0)\n",
    "            likelihood = torch.mean(torch.norm(m_i - sum_x, dim=[1, 2]))\n",
    "            if i%10 == 0: print(likelihood.item())\n",
    "            likelihood.backward()\n",
    "            \n",
    "            for xi, x in enumerate(xs):\n",
    "                xs[xi] = x - self.eta * x.grad \n",
    "                xs[xi].grad = None\n",
    "        \n",
    "        xs = [x.clamp(-1.0, 1.0) for x in xs]\n",
    "        return xs\n",
    "\n",
    "\n",
    "def separate(\n",
    "    model1,\n",
    "    model2,\n",
    "    mixture, \n",
    "    device: torch.device = torch.device(\"cuda\"), \n",
    "    num_steps:int = 100,\n",
    "):\n",
    "    \n",
    "    batch, in_channels = 1, 1\n",
    "    samples = mixture.shape[-1]\n",
    "\n",
    "    m = torch.tensor(mixture).to(device)\n",
    "    models = [model1.model, model2.model]\n",
    "    \n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "        \n",
    "    diffusion_separator = DiffusionSeparator(\n",
    "        [model.diffusion for model in models],\n",
    "        samplers=[AEulerSampler(), AEulerSampler()],\n",
    "        sigma_schedules=[\n",
    "            KarrasSchedule(sigma_min=1e-4, sigma_max=10.0, rho=7.0),\n",
    "            KarrasSchedule(sigma_min=1e-4, sigma_max=10.0, rho=7.0),\n",
    "        ],\n",
    "        num_steps=num_steps,\n",
    "    )\n",
    "    noises = [torch.randn_like(m).to(device), torch.randn_like(m).to(device)]\n",
    "    return diffusion_separator.forward(m, noises)\n",
    "y1, y2 = separate(model_1, model_2, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y1.shape\n",
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "display(Audio(m.detach().cpu().view(1,-1), rate = sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(y1.detach().cpu().view(1,-1), rate = sampling_rate))\n",
    "display(Audio(y2.detach().cpu().view(1,-1), rate = sampling_rate))\n",
    "display(Audio((y1+y2).detach().cpu().view(1,-1), rate = sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#display(Audio(torch.from_numpy(m).sum(dim=1), rate = sampling_rate))\n",
    "plt.plot(torch.tensor(m).mean(1).detach().cpu().view(-1))\n",
    "plt.plot((y1).mean(1).detach().cpu().view(-1))\n",
    "plt.plot((y2).mean(1).detach().cpu().view(-1))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "m=torch.tensor(m).cuda()\n",
    "print((y1+y2 - m).norm())\n",
    "\n",
    "sum_x = torch.stack([y1,y2]).sum(dim=0)\n",
    "likelihood = torch.mean(torch.norm(m - sum_x, dim=[1, 2]))\n",
    "print(likelihood)\n",
    "\n",
    "plt.plot(m.mean(dim=1).detach().cpu().view(-1))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
